{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "# !pip3 -qq install torch==0.4.1\n",
    "# !pip3 -qq install bokeh==0.13.0\n",
    "# !pip3 -qq install gensim==3.6.0\n",
    "# !pip3 -qq install nltk\n",
    "# !pip3 -qq install scikit-learn==0.20.2\n",
    "!pip3 -qq install torch\n",
    "!pip3 -qq install bokeh\n",
    "!pip3 -qq install gensim\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/study/mage/2_dl/dlcourse_ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /data/arch/arseniy/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /data/arch/arseniy/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'DET', 'ADP', 'VERB', 'ADJ', 'NUM', 'NOUN', '.', 'CONJ', 'PRON', 'PRT', 'ADV', 'X'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17317/3805302136.py:6: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17317/648162845.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17317/723456341.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17317/723456341.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 93.43%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))\n",
    "trigram_tagger = nltk.TrigramTagger(train_data, backoff=bigram_tagger)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self.fc = nn.Linear(in_features=lstm_hidden_dim, out_features=tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding_result = self.embedding(inputs)\n",
    "        lstm_result, _ = self.lstm(embedding_result)\n",
    "        return self.fc(lstm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.08695652335882187\n"
     ]
    }
   ],
   "source": [
    "def calc_correct_and_total_count(logits, y_batch):\n",
    "    y_pred_batch = torch.argmax(logits, -1)\n",
    "\n",
    "    mask = y_batch.bool()\n",
    "    correct_samples = torch.sum((y_pred_batch == y_batch) * mask)\n",
    "    total_samples = torch.sum(mask)\n",
    "    return correct_samples, total_samples\n",
    "\n",
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "logits = model(X_batch)\n",
    "\n",
    "correct_samples, total_samples = calc_correct_and_total_count(logits, y_batch)\n",
    "accuracy = correct_samples / total_samples\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5818397998809814\n"
     ]
    }
   ],
   "source": [
    "def calc_loss(criterion, logits, y_batch):\n",
    "    # return criterion(\n",
    "    #     torch.transpose(torch.transpose(logits, 0, 1), 1, 2),\n",
    "    #     torch.transpose(y_batch, 0, 1),\n",
    "    # )\n",
    "    return criterion(logits.reshape((-1, len(tag2ind))), y_batch.view(-1))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "loss = calc_loss(criterion, logits, y_batch)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = calc_loss(criterion, logits, y_batch)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = calc_correct_and_total_count(logits, y_batch)\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None,\n",
    "        scheduler=None, scheduler_needs_loss=False):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            if scheduler_needs_loss:\n",
    "                scheduler.step(val_loss if val_data is not None else train_loss)\n",
    "            else:\n",
    "                scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 15] Train: Loss = 0.68133, Accuracy = 78.60%: 100%|██████████| 572/572 [00:16<00:00, 35.36it/s]\n",
      "[1 / 15]   Val: Loss = 0.35993, Accuracy = 87.85%: 100%|██████████| 13/13 [00:00<00:00, 31.45it/s]\n",
      "[2 / 15] Train: Loss = 0.27214, Accuracy = 91.03%: 100%|██████████| 572/572 [00:15<00:00, 35.85it/s]\n",
      "[2 / 15]   Val: Loss = 0.23995, Accuracy = 91.98%: 100%|██████████| 13/13 [00:00<00:00, 30.90it/s]\n",
      "[3 / 15] Train: Loss = 0.18556, Accuracy = 93.85%: 100%|██████████| 572/572 [00:16<00:00, 35.54it/s]\n",
      "[3 / 15]   Val: Loss = 0.18962, Accuracy = 93.57%: 100%|██████████| 13/13 [00:00<00:00, 27.71it/s]\n",
      "[4 / 15] Train: Loss = 0.13948, Accuracy = 95.35%: 100%|██████████| 572/572 [00:16<00:00, 35.56it/s]\n",
      "[4 / 15]   Val: Loss = 0.16683, Accuracy = 94.29%: 100%|██████████| 13/13 [00:00<00:00, 30.50it/s]\n",
      "[5 / 15] Train: Loss = 0.10920, Accuracy = 96.32%: 100%|██████████| 572/572 [00:16<00:00, 35.62it/s]\n",
      "[5 / 15]   Val: Loss = 0.15448, Accuracy = 94.75%: 100%|██████████| 13/13 [00:00<00:00, 31.11it/s]\n",
      "[6 / 15] Train: Loss = 0.08757, Accuracy = 97.04%: 100%|██████████| 572/572 [00:16<00:00, 35.63it/s]\n",
      "[6 / 15]   Val: Loss = 0.14641, Accuracy = 94.95%: 100%|██████████| 13/13 [00:00<00:00, 29.34it/s]\n",
      "[7 / 15] Train: Loss = 0.07074, Accuracy = 97.62%: 100%|██████████| 572/572 [00:16<00:00, 35.36it/s]\n",
      "[7 / 15]   Val: Loss = 0.14919, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 31.53it/s]\n",
      "[8 / 15] Train: Loss = 0.05742, Accuracy = 98.05%: 100%|██████████| 572/572 [00:16<00:00, 35.51it/s]\n",
      "[8 / 15]   Val: Loss = 0.15310, Accuracy = 94.92%: 100%|██████████| 13/13 [00:00<00:00, 30.80it/s]\n",
      "[9 / 15] Train: Loss = 0.04707, Accuracy = 98.43%: 100%|██████████| 572/572 [00:16<00:00, 35.53it/s]\n",
      "[9 / 15]   Val: Loss = 0.15649, Accuracy = 95.01%: 100%|██████████| 13/13 [00:00<00:00, 30.32it/s]\n",
      "[10 / 15] Train: Loss = 0.03847, Accuracy = 98.72%: 100%|██████████| 572/572 [00:16<00:00, 35.68it/s]\n",
      "[10 / 15]   Val: Loss = 0.16301, Accuracy = 94.94%: 100%|██████████| 13/13 [00:00<00:00, 30.90it/s]\n",
      "[11 / 15] Train: Loss = 0.03128, Accuracy = 98.97%: 100%|██████████| 572/572 [00:16<00:00, 35.55it/s]\n",
      "[11 / 15]   Val: Loss = 0.16938, Accuracy = 95.02%: 100%|██████████| 13/13 [00:00<00:00, 28.31it/s]\n",
      "[12 / 15] Train: Loss = 0.02570, Accuracy = 99.17%: 100%|██████████| 572/572 [00:16<00:00, 35.61it/s] \n",
      "[12 / 15]   Val: Loss = 0.17794, Accuracy = 94.98%: 100%|██████████| 13/13 [00:00<00:00, 31.42it/s]\n",
      "[13 / 15] Train: Loss = 0.02089, Accuracy = 99.35%: 100%|██████████| 572/572 [00:15<00:00, 35.76it/s]\n",
      "[13 / 15]   Val: Loss = 0.19451, Accuracy = 94.82%: 100%|██████████| 13/13 [00:00<00:00, 28.91it/s]\n",
      "[14 / 15] Train: Loss = 0.01714, Accuracy = 99.47%: 100%|██████████| 572/572 [00:16<00:00, 35.40it/s]\n",
      "[14 / 15]   Val: Loss = 0.21020, Accuracy = 94.76%: 100%|██████████| 13/13 [00:00<00:00, 28.64it/s]\n",
      "[15 / 15] Train: Loss = 0.01392, Accuracy = 99.58%: 100%|██████████| 572/572 [00:16<00:00, 35.60it/s] \n",
      "[15 / 15]   Val: Loss = 0.22136, Accuracy = 94.70%: 100%|██████████| 13/13 [00:00<00:00, 30.23it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())#, lr=1e-3, weight_decay=1.5e-4)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=3, verbose=True)\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=15,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512,\n",
    "    #scheduler=scheduler, scheduler_needs_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.22119, Accuracy = 94.73%: 100%|██████████| 28/28 [00:00<00:00, 32.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.22119, Accuracy = 94.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def do_test_epoch(model, criterion, test_data, test_batch_size):\n",
    "    return do_epoch(model, criterion, test_data, test_batch_size, None, 'Test:')\n",
    "\n",
    "test_loss, test_accuracy = do_test_epoch(model, criterion, (X_test, y_test), test_batch_size=512)\n",
    "print(f\"Test: Loss = {test_loss:.5f}, Accuracy = {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 15] Train: Loss = 0.55056, Accuracy = 82.66%: 100%|██████████| 572/572 [00:19<00:00, 29.14it/s]\n",
      "[1 / 15]   Val: Loss = 0.28407, Accuracy = 90.84%: 100%|██████████| 13/13 [00:00<00:00, 17.15it/s]\n",
      "[2 / 15] Train: Loss = 0.20391, Accuracy = 93.52%: 100%|██████████| 572/572 [00:19<00:00, 29.37it/s]\n",
      "[2 / 15]   Val: Loss = 0.18715, Accuracy = 93.95%: 100%|██████████| 13/13 [00:00<00:00, 18.02it/s]\n",
      "[3 / 15] Train: Loss = 0.12931, Accuracy = 96.01%: 100%|██████████| 572/572 [00:19<00:00, 29.12it/s]\n",
      "[3 / 15]   Val: Loss = 0.14506, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 17.98it/s]\n",
      "[4 / 15] Train: Loss = 0.08836, Accuracy = 97.30%: 100%|██████████| 572/572 [00:19<00:00, 29.02it/s]\n",
      "[4 / 15]   Val: Loss = 0.12928, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 18.11it/s]\n",
      "[5 / 15] Train: Loss = 0.06104, Accuracy = 98.19%: 100%|██████████| 572/572 [00:19<00:00, 29.30it/s]\n",
      "[5 / 15]   Val: Loss = 0.12422, Accuracy = 95.95%: 100%|██████████| 13/13 [00:00<00:00, 17.61it/s]\n",
      "[6 / 15] Train: Loss = 0.04191, Accuracy = 98.80%: 100%|██████████| 572/572 [00:19<00:00, 29.32it/s]\n",
      "[6 / 15]   Val: Loss = 0.12752, Accuracy = 95.93%: 100%|██████████| 13/13 [00:00<00:00, 17.41it/s]\n",
      "[7 / 15] Train: Loss = 0.02824, Accuracy = 99.23%: 100%|██████████| 572/572 [00:19<00:00, 29.04it/s]\n",
      "[7 / 15]   Val: Loss = 0.12132, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 17.70it/s]\n",
      "[8 / 15] Train: Loss = 0.01859, Accuracy = 99.52%: 100%|██████████| 572/572 [00:19<00:00, 29.12it/s] \n",
      "[8 / 15]   Val: Loss = 0.12950, Accuracy = 96.12%: 100%|██████████| 13/13 [00:00<00:00, 16.66it/s]\n",
      "[9 / 15] Train: Loss = 0.01205, Accuracy = 99.72%: 100%|██████████| 572/572 [00:19<00:00, 29.02it/s] \n",
      "[9 / 15]   Val: Loss = 0.13787, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 17.17it/s]\n",
      "[10 / 15] Train: Loss = 0.00751, Accuracy = 99.85%: 100%|██████████| 572/572 [00:19<00:00, 28.89it/s] \n",
      "[10 / 15]   Val: Loss = 0.14617, Accuracy = 96.12%: 100%|██████████| 13/13 [00:00<00:00, 17.43it/s]\n",
      "[11 / 15] Train: Loss = 0.00476, Accuracy = 99.92%: 100%|██████████| 572/572 [00:19<00:00, 29.20it/s] \n",
      "[11 / 15]   Val: Loss = 0.15214, Accuracy = 96.13%: 100%|██████████| 13/13 [00:00<00:00, 17.83it/s]\n",
      "[12 / 15] Train: Loss = 0.00300, Accuracy = 99.96%: 100%|██████████| 572/572 [00:19<00:00, 29.21it/s] \n",
      "[12 / 15]   Val: Loss = 0.16323, Accuracy = 96.05%: 100%|██████████| 13/13 [00:00<00:00, 17.45it/s]\n",
      "[13 / 15] Train: Loss = 0.00184, Accuracy = 99.98%: 100%|██████████| 572/572 [00:19<00:00, 28.91it/s] \n",
      "[13 / 15]   Val: Loss = 0.16674, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 17.07it/s]\n",
      "[14 / 15] Train: Loss = 0.00123, Accuracy = 99.99%: 100%|██████████| 572/572 [00:19<00:00, 28.96it/s] \n",
      "[14 / 15]   Val: Loss = 0.18194, Accuracy = 96.02%: 100%|██████████| 13/13 [00:00<00:00, 18.36it/s]\n",
      "[15 / 15] Train: Loss = 0.00117, Accuracy = 99.99%: 100%|██████████| 572/572 [00:19<00:00, 29.23it/s] \n",
      "[15 / 15]   Val: Loss = 0.17843, Accuracy = 96.10%: 100%|██████████| 13/13 [00:00<00:00, 18.57it/s]\n"
     ]
    }
   ],
   "source": [
    "class BidirectionalLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            bidirectional=True,\n",
    "            input_size=word_emb_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers_count,\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=2*lstm_hidden_dim, out_features=tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding_result = self.embedding(inputs)\n",
    "        lstm_result, _ = self.lstm(embedding_result)\n",
    "        return self.fc(lstm_result)\n",
    "\n",
    "bi_model = BidirectionalLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(bi_model.parameters())#, lr=1e-3, weight_decay=1e-4)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=3, verbose=True)\n",
    "\n",
    "fit(bi_model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=15,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512,\n",
    "    #scheduler=scheduler, scheduler_needs_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.17952, Accuracy = 96.16%: 100%|██████████| 28/28 [00:01<00:00, 18.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.17952, Accuracy = 96.16%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = do_test_epoch(bi_model, criterion, (X_test, y_test), test_batch_size=512)\n",
    "print(f\"Test: Loss = {test_loss:.5f}, Accuracy = {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "# Не работает на python 3.10\n",
    "# Нужно патчить импорт в gensim, его авторы не знают python.\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings))\n",
    "        self.lstm = nn.LSTM(\n",
    "            bidirectional=True,\n",
    "            input_size=embeddings.shape[1],\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers_count,\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=2*lstm_hidden_dim, out_features=tagset_size)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding_result = self.embedding(inputs)\n",
    "        lstm_result, _ = self.lstm(embedding_result)\n",
    "        return self.fc(lstm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.55803, Accuracy = 83.90%: 100%|██████████| 572/572 [00:06<00:00, 89.37it/s]\n",
      "[1 / 20]   Val: Loss = 0.25315, Accuracy = 92.56%: 100%|██████████| 13/13 [00:00<00:00, 34.30it/s]\n",
      "[2 / 20] Train: Loss = 0.18703, Accuracy = 94.47%: 100%|██████████| 572/572 [00:06<00:00, 84.15it/s]\n",
      "[2 / 20]   Val: Loss = 0.17280, Accuracy = 94.73%: 100%|██████████| 13/13 [00:00<00:00, 34.65it/s]\n",
      "[3 / 20] Train: Loss = 0.13368, Accuracy = 96.01%: 100%|██████████| 572/572 [00:06<00:00, 87.62it/s]\n",
      "[3 / 20]   Val: Loss = 0.14050, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 34.24it/s]\n",
      "[4 / 20] Train: Loss = 0.10804, Accuracy = 96.73%: 100%|██████████| 572/572 [00:06<00:00, 87.53it/s]\n",
      "[4 / 20]   Val: Loss = 0.12356, Accuracy = 96.15%: 100%|██████████| 13/13 [00:00<00:00, 33.38it/s]\n",
      "[5 / 20] Train: Loss = 0.09301, Accuracy = 97.15%: 100%|██████████| 572/572 [00:06<00:00, 85.50it/s]\n",
      "[5 / 20]   Val: Loss = 0.11308, Accuracy = 96.44%: 100%|██████████| 13/13 [00:00<00:00, 34.10it/s]\n",
      "[6 / 20] Train: Loss = 0.08259, Accuracy = 97.46%: 100%|██████████| 572/572 [00:06<00:00, 86.73it/s]\n",
      "[6 / 20]   Val: Loss = 0.10699, Accuracy = 96.61%: 100%|██████████| 13/13 [00:00<00:00, 32.71it/s]\n",
      "[7 / 20] Train: Loss = 0.07493, Accuracy = 97.70%: 100%|██████████| 572/572 [00:06<00:00, 86.51it/s]\n",
      "[7 / 20]   Val: Loss = 0.10150, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 33.98it/s]\n",
      "[8 / 20] Train: Loss = 0.06933, Accuracy = 97.86%: 100%|██████████| 572/572 [00:06<00:00, 85.31it/s]\n",
      "[8 / 20]   Val: Loss = 0.09783, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 32.80it/s]\n",
      "[9 / 20] Train: Loss = 0.06414, Accuracy = 98.01%: 100%|██████████| 572/572 [00:06<00:00, 85.35it/s]\n",
      "[9 / 20]   Val: Loss = 0.10061, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 33.88it/s]\n",
      "[10 / 20] Train: Loss = 0.06007, Accuracy = 98.13%: 100%|██████████| 572/572 [00:06<00:00, 84.84it/s]\n",
      "[10 / 20]   Val: Loss = 0.09681, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 32.57it/s]\n",
      "[11 / 20] Train: Loss = 0.05630, Accuracy = 98.25%: 100%|██████████| 572/572 [00:06<00:00, 86.18it/s] \n",
      "[11 / 20]   Val: Loss = 0.09510, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 33.71it/s]\n",
      "[12 / 20] Train: Loss = 0.05326, Accuracy = 98.33%: 100%|██████████| 572/572 [00:06<00:00, 84.55it/s]\n",
      "[12 / 20]   Val: Loss = 0.09245, Accuracy = 97.03%: 100%|██████████| 13/13 [00:00<00:00, 32.72it/s]\n",
      "[13 / 20] Train: Loss = 0.05045, Accuracy = 98.42%: 100%|██████████| 572/572 [00:06<00:00, 82.82it/s]\n",
      "[13 / 20]   Val: Loss = 0.09203, Accuracy = 97.09%: 100%|██████████| 13/13 [00:00<00:00, 34.16it/s]\n",
      "[14 / 20] Train: Loss = 0.04787, Accuracy = 98.49%: 100%|██████████| 572/572 [00:06<00:00, 84.02it/s]\n",
      "[14 / 20]   Val: Loss = 0.09296, Accuracy = 97.06%: 100%|██████████| 13/13 [00:00<00:00, 34.49it/s]\n",
      "[15 / 20] Train: Loss = 0.04567, Accuracy = 98.57%: 100%|██████████| 572/572 [00:07<00:00, 81.64it/s]\n",
      "[15 / 20]   Val: Loss = 0.09237, Accuracy = 97.06%: 100%|██████████| 13/13 [00:00<00:00, 33.79it/s]\n",
      "[16 / 20] Train: Loss = 0.04332, Accuracy = 98.65%: 100%|██████████| 572/572 [00:06<00:00, 83.85it/s]\n",
      "[16 / 20]   Val: Loss = 0.09293, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 34.13it/s]\n",
      "[17 / 20] Train: Loss = 0.04141, Accuracy = 98.70%: 100%|██████████| 572/572 [00:06<00:00, 84.95it/s]\n",
      "[17 / 20]   Val: Loss = 0.09337, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 35.71it/s]\n",
      "[18 / 20] Train: Loss = 0.03952, Accuracy = 98.76%: 100%|██████████| 572/572 [00:06<00:00, 83.47it/s]\n",
      "[18 / 20]   Val: Loss = 0.09260, Accuracy = 97.08%: 100%|██████████| 13/13 [00:00<00:00, 34.28it/s]\n",
      "[19 / 20] Train: Loss = 0.03791, Accuracy = 98.82%: 100%|██████████| 572/572 [00:07<00:00, 75.02it/s]\n",
      "[19 / 20]   Val: Loss = 0.09704, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 33.22it/s]\n",
      "[20 / 20] Train: Loss = 0.03594, Accuracy = 98.88%: 100%|██████████| 572/572 [00:06<00:00, 83.20it/s]\n",
      "[20 / 20]   Val: Loss = 0.09625, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 34.17it/s]\n"
     ]
    }
   ],
   "source": [
    "bi_pre_model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(bi_pre_model.parameters())\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=3, verbose=True)\n",
    "\n",
    "fit(bi_pre_model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512,    \n",
    "    #scheduler=scheduler, scheduler_needs_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.09543, Accuracy = 97.04%: 100%|██████████| 28/28 [00:00<00:00, 36.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.09543, Accuracy = 97.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = do_test_epoch(bi_pre_model, criterion, (X_test, y_test), test_batch_size=512)\n",
    "print(f\"Test: Loss = {test_loss:.5f}, Accuracy = {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
